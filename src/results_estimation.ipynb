{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004f7a3a",
   "metadata": {},
   "source": [
    "# This is where I estimate baseline, naive, and ppi corrected inference parameters for each model classic, bert and gpt_zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "049369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# load custom local packages\n",
    "sys.path.append(\"C:\\\\Users\\\\Adam\\\\Desktop\\\\code_projects\\\\GitHub\\\\va_nlp\\\\utils\")\n",
    "\n",
    "from dataset_utils import dataframe_decorator\n",
    "from statistics_utils import *\n",
    "from ppi_plusplus_multi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac15d1",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../src/gpt_nlp/results_df.csv')\n",
    "mexico_knn = pd.read_csv('../data/results/mexico_KNN.csv')\n",
    "mexico_svm = pd.read_csv('../data/results/mexico_SVM.csv')\n",
    "mexico_nb = pd.read_csv('../data/results/mexico_NB.csv')\n",
    "mexico_bert = pd.read_csv('../data/results/mexico_bert.csv')\n",
    "mexico_gpt4zs = pd.read_csv('../data/results/mexico_gpt4_zs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc607",
   "metadata": {},
   "source": [
    "# Break into 80/20 split for Naive and PPI estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46086d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(input_df, test_size=0.2):\n",
    "    '''\n",
    "    Takes input df which has three columns, [Y, X, Y_hat]\n",
    "    Subsets df to exclude 'unclassified' from Y_hat \n",
    "    returns data split unlabeled/labeled with a default 80/20 train/test split:\n",
    "        Y_sorted (ndarray): All gold-standard labels, sorted. \n",
    "        X_sorted (ndarray): All covariates corresponding to the gold-standard labels, sorted. \n",
    "        Y_lab (ndarray) : test_size number of gold standard labels.\n",
    "        Yhat_lab (ndarray): test_size number of predictions corresponding to the gold-standard labels.\n",
    "        X_lab (ndarray) : test_size number of covariates corresponding to the gold-standard labels.        \n",
    "        Yhat_unlabeled (ndarray): (1-test_size) number of predictions corresponding to the gold-standard labels.\n",
    "        X_unlabeled (ndarray): (1-test_size) number of covariates corresponding to the unlabeled data.\n",
    "        \n",
    "           \n",
    "    '''\n",
    "    \n",
    "    # subset to drop unclassified\n",
    "    subset = input_df[input_df['Y_hat']!='unclassified']\n",
    "\n",
    "    # 80/20 split on entire dataframe into unabeled and labeled subsets\n",
    "    unlab_df, lab_df = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # separate Y's and X's\n",
    "    # full Y and X\n",
    "    Y = input_df['Y'].to_numpy()\n",
    "    X = input_df['X'].to_numpy()\n",
    "    \n",
    "    # labeled Y, X, Y_hat\n",
    "    Y_lab = lab_df['Y'].to_numpy()\n",
    "    X_lab = lab_df['X'].to_numpy()\n",
    "    Yhat_lab = lab_df['Y_hat'].to_numpy()\n",
    "    \n",
    "    # unlabeled Y, X, Y_hat\n",
    "    Y_unlab = unlab_df['Y'].to_numpy()\n",
    "    X_unlab = unlab_df['X'].to_numpy()\n",
    "    Yhat_unlab = unlab_df['Y_hat'].to_numpy()\n",
    "    \n",
    "    # combine 20/80 labeled unlabeled\n",
    "    Y_combined = np.append(Y_lab, Y_unlab)\n",
    "    X_combined = np.append(X_lab, X_unlab)\n",
    "    \n",
    "    # transpose X's\n",
    "    X = X.reshape(-1,1)\n",
    "    X_lab = X_lab.reshape(-1,1)\n",
    "    X_unlab = X_unlab.reshape(-1,1)\n",
    "    X_combined = X_combined.reshape(-1,1)\n",
    "    \n",
    "    # sort for MNLogit so that 0 is the left out reference category\n",
    "    sort_idx = np.argsort(Y)\n",
    "    Y_sorted = Y[sort_idx]\n",
    "    X_sorted = X[sort_idx]\n",
    "    Y_combined_sorted = Y_combined[sort_idx]\n",
    "    X_combined_sorted = X_combined[sort_idx]\n",
    "    \n",
    "    return Y_sorted, X_sorted, Y_lab, Yhat_lab, X_lab, Yhat_unlab, X_unlab, Y_combined_sorted, X_combined_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33f5b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sorted, X_sorted, Y_lab, Yhat_lab, X_lab, Yhat_unlab, X_unlab, Y_combined_sorted, X_combined_sorted = data_split(mexico_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ca1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.961287\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>MNLogit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1306</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>  <td>  1302</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 20 Mar 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:27:11</td>     <th>  Log-Likelihood:    </th> <td> -1255.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1333.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>   nan</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>y=1</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0125</td> <td>    0.003</td> <td>    4.846</td> <td> 0.000</td> <td>    0.007</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=2</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0040</td> <td>    0.003</td> <td>    1.404</td> <td> 0.160</td> <td>   -0.002</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=3</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>   -0.0294</td> <td>    0.005</td> <td>   -5.709</td> <td> 0.000</td> <td>   -0.039</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=4</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0429</td> <td>    0.002</td> <td>   19.309</td> <td> 0.000</td> <td>    0.039</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        y         & \\textbf{  No. Observations:  } &     1306    \\\\\n",
       "\\textbf{Model:}           &     MNLogit      & \\textbf{  Df Residuals:      } &     1302    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        0    \\\\\n",
       "\\textbf{Date:}            & Wed, 20 Mar 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05837    \\\\\n",
       "\\textbf{Time:}            &     11:27:11     & \\textbf{  Log-Likelihood:    } &   -1255.4   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -1333.3   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } &      nan    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=1} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0125  &        0.003     &     4.846  &         0.000        &        0.007    &        0.018     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=2} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0040  &        0.003     &     1.404  &         0.160        &       -0.002    &        0.010     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=3} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &      -0.0294  &        0.005     &    -5.709  &         0.000        &       -0.039    &       -0.019     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=4} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0429  &        0.002     &    19.309  &         0.000        &        0.039    &        0.047     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{MNLogit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          MNLogit Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1306\n",
       "Model:                        MNLogit   Df Residuals:                     1302\n",
       "Method:                           MLE   Df Model:                            0\n",
       "Date:                Wed, 20 Mar 2024   Pseudo R-squ.:                 0.05837\n",
       "Time:                        11:27:11   Log-Likelihood:                -1255.4\n",
       "converged:                       True   LL-Null:                       -1333.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                       nan\n",
       "==============================================================================\n",
       "       y=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0125      0.003      4.846      0.000       0.007       0.018\n",
       "------------------------------------------------------------------------------\n",
       "       y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0040      0.003      1.404      0.160      -0.002       0.010\n",
       "------------------------------------------------------------------------------\n",
       "       y=3       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.0294      0.005     -5.709      0.000      -0.039      -0.019\n",
       "------------------------------------------------------------------------------\n",
       "       y=4       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0429      0.002     19.309      0.000       0.039       0.047\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Regression\n",
    "mn_logit_baseline = sm.MNLogit(Y_sorted, X_sorted)\n",
    "mn_logit_baseline_res = mn_logit_baseline.fit(method = \"newton\", full_output = True)\n",
    "mn_logit_baseline_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb23cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.961287\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>MNLogit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1306</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>  <td>  1302</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 20 Mar 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:27:20</td>     <th>  Log-Likelihood:    </th> <td> -1255.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1333.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>   nan</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>y=1</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0125</td> <td>    0.003</td> <td>    4.846</td> <td> 0.000</td> <td>    0.007</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=2</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0040</td> <td>    0.003</td> <td>    1.404</td> <td> 0.160</td> <td>   -0.002</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=3</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>   -0.0294</td> <td>    0.005</td> <td>   -5.709</td> <td> 0.000</td> <td>   -0.039</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y=4</th>    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0429</td> <td>    0.002</td> <td>   19.309</td> <td> 0.000</td> <td>    0.039</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        y         & \\textbf{  No. Observations:  } &     1306    \\\\\n",
       "\\textbf{Model:}           &     MNLogit      & \\textbf{  Df Residuals:      } &     1302    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        0    \\\\\n",
       "\\textbf{Date:}            & Wed, 20 Mar 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05837    \\\\\n",
       "\\textbf{Time:}            &     11:27:20     & \\textbf{  Log-Likelihood:    } &   -1255.4   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -1333.3   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } &      nan    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=1} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0125  &        0.003     &     4.846  &         0.000        &        0.007    &        0.018     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=2} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0040  &        0.003     &     1.404  &         0.160        &       -0.002    &        0.010     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=3} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &      -0.0294  &        0.005     &    -5.709  &         0.000        &       -0.039    &       -0.019     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "\\textbf{y=4} & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{x1}  &       0.0429  &        0.002     &    19.309  &         0.000        &        0.039    &        0.047     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{MNLogit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          MNLogit Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1306\n",
       "Model:                        MNLogit   Df Residuals:                     1302\n",
       "Method:                           MLE   Df Model:                            0\n",
       "Date:                Wed, 20 Mar 2024   Pseudo R-squ.:                 0.05837\n",
       "Time:                        11:27:20   Log-Likelihood:                -1255.4\n",
       "converged:                       True   LL-Null:                       -1333.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                       nan\n",
       "==============================================================================\n",
       "       y=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0125      0.003      4.846      0.000       0.007       0.018\n",
       "------------------------------------------------------------------------------\n",
       "       y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0040      0.003      1.404      0.160      -0.002       0.010\n",
       "------------------------------------------------------------------------------\n",
       "       y=3       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.0294      0.005     -5.709      0.000      -0.039      -0.019\n",
       "------------------------------------------------------------------------------\n",
       "       y=4       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0429      0.002     19.309      0.000       0.039       0.047\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Regression\n",
    "mn_logit_baseline = sm.MNLogit(Y_combined_sorted, X_combined_sorted)\n",
    "mn_logit_baseline_res = mn_logit_baseline.fit(method = \"newton\", full_output = True)\n",
    "mn_logit_baseline_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95518fad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 262 is out of bounds for axis 0 with size 262",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m theta_ppi_ci \u001b[38;5;241m=\u001b[39m ppi_multiclass_logistic_ci(\n\u001b[0;32m      2\u001b[0m             X\u001b[38;5;241m=\u001b[39mX_sorted,\n\u001b[0;32m      3\u001b[0m             Y\u001b[38;5;241m=\u001b[39mY_sorted,\n\u001b[0;32m      4\u001b[0m             Yhat\u001b[38;5;241m=\u001b[39mYhat_lab,\n\u001b[0;32m      5\u001b[0m             X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlab,\n\u001b[0;32m      6\u001b[0m             Yhat_unlabeled\u001b[38;5;241m=\u001b[39mYhat_unlab,\n\u001b[0;32m      7\u001b[0m             optimizer_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1000\u001b[39m},\n\u001b[0;32m      8\u001b[0m         )\n",
      "File \u001b[1;32m~\\Desktop\\code_projects\\GitHub\\va_nlp\\utils\\ppi_plusplus_multi.py:393\u001b[0m, in \u001b[0;36mppi_multiclass_logistic_ci\u001b[1;34m(X, Y, Yhat, X_unlabeled, Yhat_unlabeled, alpha, alternative, lhat, coord, optimizer_options)\u001b[0m\n\u001b[0;32m    388\u001b[0m df \u001b[38;5;241m=\u001b[39m (K\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39md \n\u001b[0;32m    390\u001b[0m use_unlabeled \u001b[38;5;241m=\u001b[39m lhat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 393\u001b[0m ppi_pointest \u001b[38;5;241m=\u001b[39m ppi_multi_class_pointestimate(\n\u001b[0;32m    394\u001b[0m     X,\n\u001b[0;32m    395\u001b[0m     Y,\n\u001b[0;32m    396\u001b[0m     Yhat,\n\u001b[0;32m    397\u001b[0m     X_unlabeled,\n\u001b[0;32m    398\u001b[0m     Yhat_unlabeled,\n\u001b[0;32m    399\u001b[0m     lhat\u001b[38;5;241m=\u001b[39mlhat,\n\u001b[0;32m    400\u001b[0m     coord\u001b[38;5;241m=\u001b[39mcoord,\n\u001b[0;32m    401\u001b[0m     optimizer_options\u001b[38;5;241m=\u001b[39moptimizer_options,\n\u001b[0;32m    402\u001b[0m \n\u001b[0;32m    403\u001b[0m )\n\u001b[0;32m    405\u001b[0m grads, grads_hat, grads_hat_unlabeled, hessian, inv_hessian \u001b[38;5;241m=\u001b[39m _multiclass_ci_get_stats(\n\u001b[0;32m    406\u001b[0m     ppi_pointest,\n\u001b[0;32m    407\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m     use_unlabeled\u001b[38;5;241m=\u001b[39muse_unlabeled,\n\u001b[0;32m    413\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\code_projects\\GitHub\\va_nlp\\utils\\ppi_plusplus_multi.py:155\u001b[0m, in \u001b[0;36mppi_multi_class_pointestimate\u001b[1;34m(X, Y, Yhat, X_unlabeled, Yhat_unlabeled, lhat, coord, optimizer_options)\u001b[0m\n\u001b[0;32m    152\u001b[0m lhat_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lhat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lhat\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# optimization over (K-1)*d degrees of freedom by forcing reference class parameters to zero\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m ppi_pointest_extra \u001b[38;5;241m=\u001b[39m minimize(\n\u001b[0;32m    156\u001b[0m     fun\u001b[38;5;241m=\u001b[39mrectified_multiclass_logistic_loss, \n\u001b[0;32m    157\u001b[0m     x0\u001b[38;5;241m=\u001b[39mtheta_0,\n\u001b[0;32m    158\u001b[0m     jac\u001b[38;5;241m=\u001b[39mrectified_multiclass_logistic_grad,\n\u001b[0;32m    159\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    160\u001b[0m     tol\u001b[38;5;241m=\u001b[39moptimizer_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    161\u001b[0m     options\u001b[38;5;241m=\u001b[39moptimizer_options)\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# remove the reference class parameters\u001b[39;00m\n\u001b[0;32m    164\u001b[0m ppi_pointest \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(theta_2d(ppi_pointest_extra, K), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(order \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    308\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mnew_bounds,\n\u001b[0;32m    309\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\Desktop\\code_projects\\GitHub\\va_nlp\\utils\\ppi_plusplus_multi.py:78\u001b[0m, in \u001b[0;36mppi_multi_class_pointestimate.<locals>.rectified_multiclass_logistic_loss\u001b[1;34m(_theta)\u001b[0m\n\u001b[0;32m     75\u001b[0m Ey \u001b[38;5;241m=\u001b[39m EY[y,:,:]\n\u001b[0;32m     76\u001b[0m loss0 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m-\u001b[39m(Xi \u001b[38;5;241m@\u001b[39m Ey) \u001b[38;5;241m@\u001b[39m _theta \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(Xi \u001b[38;5;241m@\u001b[39m EY \u001b[38;5;241m@\u001b[39m _theta)))\n\u001b[1;32m---> 78\u001b[0m yhat \u001b[38;5;241m=\u001b[39m Yhat[i]\n\u001b[0;32m     79\u001b[0m Eyhat \u001b[38;5;241m=\u001b[39m EY[yhat,:,:]\n\u001b[0;32m     80\u001b[0m loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(Xi \u001b[38;5;241m@\u001b[39m Eyhat) \u001b[38;5;241m@\u001b[39m _theta \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(Xi \u001b[38;5;241m@\u001b[39m EY \u001b[38;5;241m@\u001b[39m _theta)))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 262 is out of bounds for axis 0 with size 262"
     ]
    }
   ],
   "source": [
    "theta_ppi_ci = ppi_multiclass_logistic_ci(\n",
    "            X=X_sorted,\n",
    "            Y=Y_sorted,\n",
    "            Yhat=Yhat_lab,\n",
    "            X_unlabeled=X_unlab,\n",
    "            Yhat_unlabeled=Yhat_unlab,\n",
    "            optimizer_options = {'disp': True, 'maxiter':1000},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d119c6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b9844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1c801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09f4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb9d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841b034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fbee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2055d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a8742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d203c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649a5093",
   "metadata": {},
   "source": [
    "# Site: geographic regions (mexico, ap, up, dar, bohol, pemba)\n",
    "# Model: ai prediction model (classic, BERT, GPT4)\n",
    "# Inference: type of inference (Baseline, Naive, PPI++)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7d96f",
   "metadata": {},
   "source": [
    "# Loop through all permutations, compute PE and CI, save results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df['site'].unique()\n",
    "models = ['KNN', 'SVM', 'NB', 'bert', 'gpt4_zs']\n",
    "inferences = ['baseline', 'naive', 'ppi_plus_plus']\n",
    "results_list = []\n",
    "column_names=  [\n",
    "    'site', 'model', 'inference',\n",
    "    'baseline_pe', 'baseline_lb', 'baseline_ub',\n",
    "    'naive_pe', 'naive_lb', 'naive_ub',\n",
    "    'ppi_plus_plus_pe', 'ppi_plus_plus_lb', 'ppi_plus_plus_ub'\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "for site in sites:\n",
    "    for model in models:\n",
    "        for inference in inferences:\n",
    "            # read in dataframe for site and model\n",
    "            load_df = pd.read_csv(f'../data/results/{site}_{model}.csv')\n",
    "            \n",
    "            # baseline predictions: Y_lab ~ X_lab\n",
    "            baseline_pe = 1\n",
    "            baseline_lb = 2\n",
    "            baseline_ub = 3\n",
    "            \n",
    "            # split data 80/20 into labeled and unlabeled for naive and ppi++ inference\n",
    "            X_lab, Y_lab, Yhat_lab, X_unlab, Yhat_unlab = data_split(load_df)\n",
    "            \n",
    "            # naive predictions: 20% Y_lab,X_lab and 80% Y_unlab,X_unlab\n",
    "            naive_pe = 4\n",
    "            naive_lb = 5\n",
    "            naive_ub = 6\n",
    "            \n",
    "            # ppi++ predictions: 20% Y_lab,X_lab and 80% Y_unlab,X_unlab\n",
    "            ppi_plus_plus_pe = 7\n",
    "            ppi_plus_plus_lb = 8\n",
    "            ppi_plus_plus_ub = 9\n",
    "            \n",
    "            result = [\n",
    "                site, model, inference,\n",
    "                baseline_pe, baseline_lb, baseline_ub,\n",
    "                naive_pe, naive_lb, naive_ub,\n",
    "                ppi_plus_plus_pe, ppi_plus_plus_lb, ppi_plus_plus_ub\n",
    "            ]\n",
    "            \n",
    "            results_list.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=column_names)\n",
    "\n",
    "# write to results folder\n",
    "results_df.to_csv('../data/results/estimation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select PE, LB and UB for given site and model and inference\n",
    "site = 'mexico'\n",
    "model = 'KNN'\n",
    "inference = 'baseline'\n",
    "\n",
    "results_df[(results_df['site']==site) & (results_df['model']==model)][['site',\n",
    "                                                                       'model',\n",
    "                                                                       'inference',\n",
    "                                                                       f'{inference}_pe', \n",
    "                                                                       f'{inference}_lb', \n",
    "                                                                       f'{inference}_ub']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb7efc",
   "metadata": {},
   "source": [
    "# For each site/model permutation, plot the PE + CI for the three estimation procedures for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf735e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45c004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c58453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204820f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be000ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f8f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16932fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 0 for Classical Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X = X_lab,\n",
    "    Y = Y_lab,\n",
    "    Yhat = Yhat_lab,\n",
    "    X_unlabeled = X_unlab,\n",
    "    Yhat_unlabeled = Yhat_unlab,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19033bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 1 for Classical PPI Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 1,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = None for PPI++ Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ace3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pizza_eater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b404219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
