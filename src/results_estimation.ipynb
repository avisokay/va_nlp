{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004f7a3a",
   "metadata": {},
   "source": [
    "# This is where I estimate baseline, naive, and ppi corrected inference parameters for each model classic, bert and gpt_zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# load custom local packages\n",
    "# sys.path.append(\"C:\\\\Users\\\\Adam\\\\Desktop\\\\code_projects\\\\GitHub\\\\va_nlp\\\\utils\")\n",
    "sys.path.append('/Users/adam/Desktop/Github/va_nlp/utils')\n",
    "\n",
    "from dataset_utils import dataframe_decorator\n",
    "from statistics_utils import *\n",
    "from ppi_plusplus_multi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac15d1",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../src/gpt_nlp/results_df.csv')\n",
    "mexico_knn = pd.read_csv('../data/results/mexico_KNN.csv')\n",
    "mexico_svm = pd.read_csv('../data/results/mexico_SVM.csv')\n",
    "mexico_nb = pd.read_csv('../data/results/mexico_NB.csv')\n",
    "mexico_bert = pd.read_csv('../data/results/mexico_bert.csv')\n",
    "mexico_gpt4zs = pd.read_csv('../data/results/mexico_gpt4_zs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb6704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_score = {\n",
    "    'aids-tb': 0,\n",
    "    'communicable': 1,\n",
    "    'external': 2,\n",
    "    'maternal': 3, \n",
    "    'non-communicable': 4, \n",
    "    'unclassified': 'unclassified'\n",
    "}\n",
    "\n",
    "cod_labels = ['aids-tb', 'communicable', 'external', 'maternal', 'non-communicable']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc607",
   "metadata": {},
   "source": [
    "# Break into 80/20 split for Naive and PPI estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46086d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(input_df, test_size=0.2):\n",
    "    '''\n",
    "    Takes input df which has three columns, [Y, X, Y_hat]\n",
    "    Subsets df to exclude 'unclassified' from Y_hat \n",
    "    returns data split unlabeled/labeled with a default 80/20 train/test split:\n",
    "        Y_sorted (ndarray): All gold-standard labels, sorted. \n",
    "        X_sorted (ndarray): All covariates corresponding to the gold-standard labels, sorted. \n",
    "        Y_lab (ndarray) : test_size number of gold standard labels.\n",
    "        Yhat_lab (ndarray): test_size number of predictions corresponding to the gold-standard labels.\n",
    "        X_lab (ndarray) : test_size number of covariates corresponding to the gold-standard labels.        \n",
    "        Yhat_unlabeled (ndarray): (1-test_size) number of predictions corresponding to the gold-standard labels.\n",
    "        X_unlabeled (ndarray): (1-test_size) number of covariates corresponding to the unlabeled data.\n",
    "             \n",
    "    '''\n",
    "    \n",
    "    # subset to drop unclassified\n",
    "    subset = input_df[input_df['Y_hat']!='unclassified'].astype(int)\n",
    "\n",
    "    # split the df into test_size (labeled) and 1-test_size (unlabeled)\n",
    "    # random\n",
    "    np.random.seed(123)\n",
    "    labeled_df = subset.sample(frac=test_size)\n",
    "    unlabeled_df = subset.drop(labeled_df.index)\n",
    "    \n",
    "    # separate Y's and X's\n",
    "    # full Y and X\n",
    "    Y = subset['Y'].to_numpy()\n",
    "    X = subset['X'].to_numpy()\n",
    "    X = X.reshape(-1,1)\n",
    "    \n",
    "    # labeled Y, X, Y_hat\n",
    "    Y_lab = labeled_df['Y'].to_numpy()\n",
    "    X_lab = labeled_df['X'].to_numpy()\n",
    "    X_lab = X_lab.reshape(-1,1)\n",
    "    Yhat_lab = labeled_df['Y_hat'].to_numpy()\n",
    "    \n",
    "    # unlabeled Y, X, Y_hat\n",
    "    Y_unlab = unlabeled_df['Y'].to_numpy()\n",
    "    X_unlab = unlabeled_df['X'].to_numpy()\n",
    "    X_unlab = X_unlab.reshape(-1,1)\n",
    "    Yhat_unlab = unlabeled_df['Y_hat'].to_numpy()\n",
    "    \n",
    "    # combine 20/80 labeled unlabeled\n",
    "    X_combined = np.concatenate((X, X_unlab))\n",
    "    Y_combined = np.concatenate((Y, Yhat_unlab))\n",
    "    X_combined = X_combined.reshape(-1,1)\n",
    "    \n",
    "    # sort for MNLogit so that 0 is the left out reference category    \n",
    "    sort_idx1 = np.argsort(Y)\n",
    "    Y_sorted = Y[sort_idx1]\n",
    "    X_sorted = X[sort_idx1]\n",
    "    sort_idx2 = np.argsort(Y_lab)\n",
    "\n",
    "    Y_combined_sorted = Y_combined[sort_idx2]\n",
    "    X_combined_sorted = X_combined[sort_idx2]\n",
    "    \n",
    "    return Y_sorted, X_sorted, Y_lab, Yhat_lab, X_lab, Yhat_unlab, X_unlab, Y_combined_sorted, X_combined_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad597e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, test_size=0.2):\n",
    "    '''\n",
    "    Takes input df which has three columns, [Y, X, Y_hat]\n",
    "    Subsets df to exclude 'unclassified' from Y_hat \n",
    "    returns data split unlabeled/labeled with a default 80/20 train/test split:\n",
    "        Y_sorted (ndarray): All gold-standard labels, sorted. \n",
    "        X_sorted (ndarray): All covariates corresponding to the gold-standard labels, sorted. \n",
    "        Y_lab (ndarray) : test_size number of gold standard labels.\n",
    "        Yhat_lab (ndarray): test_size number of predictions corresponding to the gold-standard labels.\n",
    "        X_lab (ndarray) : test_size number of covariates corresponding to the gold-standard labels.        \n",
    "        Yhat_unlabeled (ndarray): (1-test_size) number of predictions corresponding to the gold-standard labels.\n",
    "        X_unlabeled (ndarray): (1-test_size) number of covariates corresponding to the unlabeled data.\n",
    "             \n",
    "    '''\n",
    "    \n",
    "    # subset to exclude unclassified predictions\n",
    "    df = df[df['Y_hat']!='unclassified'].astype(int)\n",
    "    \n",
    "    # random split\n",
    "    np.random.seed(123)\n",
    "    labeled_df = df.sample(frac=0.2)\n",
    "    unlabeled_df = df.drop(labeled_df.index)\n",
    "    \n",
    "    Y = df['Y'].to_numpy()\n",
    "    X = df['X'].to_numpy()\n",
    "    X = X.reshape(-1,1)\n",
    "    Yhat = df['Y_hat'].to_numpy()\n",
    "\n",
    "    print(Y.shape)\n",
    "\n",
    "    sort_idx1 = np.argsort(Y)\n",
    "    Y_sorted = Y[sort_idx1]\n",
    "    X_sorted = X[sort_idx1]\n",
    "    Yhat_sorted = Yhat[sort_idx1]\n",
    "\n",
    "    # Baseline regression ALL Y on ALL X\n",
    "    mn_logit = sm.MNLogit(Y_sorted, X_sorted)\n",
    "    mn_logit_res = mn_logit.fit(method = \"newton\", full_output = True)\n",
    "    print(mn_logit_res.params)\n",
    "    print(mn_logit_res.conf_int())\n",
    "        \n",
    "    Y_lab = labeled_df['Y'].to_numpy()\n",
    "    X_lab = labeled_df['X'].to_numpy()\n",
    "    X_lab = X_lab.reshape(-1,1)\n",
    "    Yhat_lab = labeled_df['Y_hat'].to_numpy()\n",
    "    Y_unlab = unlabeled_df['Y'].to_numpy()\n",
    "    X_unlab = unlabeled_df['X'].to_numpy()\n",
    "    X_unlab = X_unlab.reshape(-1,1)\n",
    "    Yhat_unlab = unlabeled_df['Y_hat'].to_numpy()\n",
    "        \n",
    "    Y_full = np.concatenate((Y_lab, Yhat_unlab))\n",
    "    X_full = np.concatenate((X_lab, X_unlab))\n",
    "    \n",
    "    print(Y_lab.shape)\n",
    "    print(Y_full.shape)\n",
    "\n",
    "    sort_idx2 = np.argsort(Y_full) # Sort using Y_lab or Y_full???? IS THIS CORRECT?? DOUBLE CHECK !!!!\n",
    "    Y_full_sorted = Y_full[sort_idx2]\n",
    "    X_full_sorted = X_full[sort_idx2]\n",
    "    \n",
    "    mn_logit = sm.MNLogit(Y_full_sorted, X_full_sorted)\n",
    "    mn_logit_res = mn_logit.fit(method = \"newton\", full_output = True)\n",
    "    print(mn_logit_res.params)\n",
    "    print(mn_logit_res.conf_int())\n",
    "\n",
    "#     mn_logit_res.summary()\n",
    "\n",
    "    return Y_sorted, X_sorted, Yhat_sorted, Y_lab, X_lab, Yhat_lab, Y_unlab, X_unlab, Yhat_unlab, Y_full_sorted, X_full_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d48853",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.961287\n",
      "         Iterations 7\n",
      "[[ 0.01251579  0.00399583 -0.02938669  0.04290408]]\n",
      "[[[ 0.00745418  0.0175774 ]]\n",
      "\n",
      " [[-0.00158093  0.00957259]]\n",
      "\n",
      " [[-0.03947504 -0.01929834]]\n",
      "\n",
      " [[ 0.03854903  0.04725912]]]\n",
      "(261,)\n",
      "(1306,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.818104\n",
      "         Iterations 7\n",
      "[[0.01972783 0.06115925 0.01225632 0.08184039]]\n",
      "[[[ 0.00652206  0.0329336 ]]\n",
      "\n",
      " [[ 0.04965909  0.07265941]]\n",
      "\n",
      " [[-0.00175172  0.02626436]]\n",
      "\n",
      " [[ 0.07044985  0.09323093]]]\n"
     ]
    }
   ],
   "source": [
    "Y_sorted, X_sorted, Yhat_sorted, Y_lab, X_lab, Yhat_lab, Y_unlab, X_unlab, Yhat_unlab, Y_full_sorted, X_full_sorted = data_split(mexico_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1de4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.961287\n",
      "         Iterations 7\n",
      "[[ 0.01251579  0.00399583 -0.02938669  0.04290408]]\n",
      "[[[ 0.00745418  0.0175774 ]]\n",
      "\n",
      " [[-0.00158093  0.00957259]]\n",
      "\n",
      " [[-0.03947504 -0.01929834]]\n",
      "\n",
      " [[ 0.03854903  0.04725912]]]\n"
     ]
    }
   ],
   "source": [
    "# Baseline regression ALL Y on ALL X\n",
    "mn_logit = sm.MNLogit(Y_sorted, X_sorted)\n",
    "mn_logit_res = mn_logit.fit(method = \"newton\", full_output = True)\n",
    "pe = mn_logit_res.params\n",
    "ci = mn_logit_res.conf_int()\n",
    "\n",
    "mn_logit_res.summary()\n",
    "\n",
    "print(pe)\n",
    "print(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dd81bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.818104\n",
      "         Iterations 7\n",
      "[[0.01972783 0.06115925 0.01225632 0.08184039]]\n",
      "[[[ 0.00652206  0.0329336 ]]\n",
      "\n",
      " [[ 0.04965909  0.07265941]]\n",
      "\n",
      " [[-0.00175172  0.02626436]]\n",
      "\n",
      " [[ 0.07044985  0.09323093]]]\n"
     ]
    }
   ],
   "source": [
    "# Naive regression using 80/20 split unlabeled/labeled\n",
    "mn_logit = sm.MNLogit(Y_full_sorted, X_full_sorted)\n",
    "mn_logit_res = mn_logit.fit(method = \"newton\", full_output = True)\n",
    "pe = mn_logit_res.params\n",
    "ci = mn_logit_res.conf_int()\n",
    "\n",
    "mn_logit_res.summary()\n",
    "\n",
    "print(pe)\n",
    "print(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa70e738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PPI++ correction using the same 80/20 split unlabeled/labeled\n",
    "theta_ppi_ci = ppi_multiclass_logistic_ci(\n",
    "            X=X_lab,\n",
    "            Y=Y_lab,\n",
    "            Yhat=Yhat_lab,\n",
    "            X_unlabeled=X_unlab,\n",
    "            Yhat_unlabeled=Yhat_unlab,\n",
    "            optimizer_options = {'disp': True, 'maxiter':1000},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a6ae9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pointest': array([ 0.01513981, -0.00346957, -0.02031198,  0.04476595]), 'ci': (array([ 0.00278088, -0.01275375, -0.0446409 ,  0.03759806]), array([0.02749875, 0.00581461, 0.00401694, 0.05193385])), 'se': array([0.0075137 , 0.00564438, 0.01479093, 0.00435777]), 'lhat': 0.016621309078555024}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    try:\n",
    "        # PPI++ correction using the same 80/20 split unlabeled/labeled\n",
    "        theta_ppi_ci = ppi_multiclass_logistic_ci(\n",
    "            X=X_lab,\n",
    "            Y=Y_lab,\n",
    "            Yhat=Yhat_lab,\n",
    "            X_unlabeled=X_unlab,\n",
    "            Yhat_unlabeled=Yhat_unlab,\n",
    "            optimizer_options = {'disp': True, 'maxiter':1000},\n",
    "        )\n",
    "        print(theta_ppi_ci)\n",
    "    except Exception as e:\n",
    "        # Handle the exception (if needed)\n",
    "        print(\"Error:\", e)\n",
    "        # Continue with the loop\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "213c86c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00278088, -0.01275375, -0.0446409 ,  0.03759806]),\n",
       " array([0.02749875, 0.00581461, 0.00401694, 0.05193385]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_ppi_ci['ci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c09f4a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta_ppi_ci' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m theta_ppi_ci\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_ppi_ci' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649a5093",
   "metadata": {},
   "source": [
    "# Site: geographic regions (mexico, ap, up, dar, bohol, pemba)\n",
    "# Model: ai prediction model (classic, BERT, GPT4)\n",
    "# Inference: type of inference (Baseline, Naive, PPI++)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7d96f",
   "metadata": {},
   "source": [
    "# Loop through all permutations, compute PE and CI, save results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df['site'].unique()\n",
    "models = ['KNN', 'SVM', 'NB', 'bert', 'gpt4_zs']\n",
    "inferences = ['baseline', 'naive', 'ppi_plus_plus']\n",
    "results_list = []\n",
    "column_names=  [\n",
    "    'site', 'model', 'inference',\n",
    "    'baseline_pe', 'baseline_lb', 'baseline_ub',\n",
    "    'naive_pe', 'naive_lb', 'naive_ub',\n",
    "    'ppi_plus_plus_pe', 'ppi_plus_plus_lb', 'ppi_plus_plus_ub'\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "for site in sites:\n",
    "    for model in models:\n",
    "        for inference in inferences:\n",
    "            # read in dataframe for site and model\n",
    "            load_df = pd.read_csv(f'../data/results/{site}_{model}.csv')\n",
    "            \n",
    "            # baseline predictions: Y_lab ~ X_lab\n",
    "            baseline_pe = 1\n",
    "            baseline_lb = 2\n",
    "            baseline_ub = 3\n",
    "            \n",
    "            # split data 80/20 into labeled and unlabeled for naive and ppi++ inference\n",
    "            X_lab, Y_lab, Yhat_lab, X_unlab, Yhat_unlab = data_split(load_df)\n",
    "            \n",
    "            # naive predictions: 20% Y_lab,X_lab and 80% Y_unlab,X_unlab\n",
    "            naive_pe = 4\n",
    "            naive_lb = 5\n",
    "            naive_ub = 6\n",
    "            \n",
    "            # ppi++ predictions: 20% Y_lab,X_lab and 80% Y_unlab,X_unlab\n",
    "            ppi_plus_plus_pe = 7\n",
    "            ppi_plus_plus_lb = 8\n",
    "            ppi_plus_plus_ub = 9\n",
    "            \n",
    "            result = [\n",
    "                site, model, inference,\n",
    "                baseline_pe, baseline_lb, baseline_ub,\n",
    "                naive_pe, naive_lb, naive_ub,\n",
    "                ppi_plus_plus_pe, ppi_plus_plus_lb, ppi_plus_plus_ub\n",
    "            ]\n",
    "            \n",
    "            results_list.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=column_names)\n",
    "\n",
    "# write to results folder\n",
    "results_df.to_csv('../data/results/estimation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select PE, LB and UB for given site and model and inference\n",
    "site = 'mexico'\n",
    "model = 'KNN'\n",
    "inference = 'baseline'\n",
    "\n",
    "results_df[(results_df['site']==site) & (results_df['model']==model)][['site',\n",
    "                                                                       'model',\n",
    "                                                                       'inference',\n",
    "                                                                       f'{inference}_pe', \n",
    "                                                                       f'{inference}_lb', \n",
    "                                                                       f'{inference}_ub']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb7efc",
   "metadata": {},
   "source": [
    "# For each site/model permutation, plot the PE + CI for the three estimation procedures for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf735e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45c004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c58453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204820f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be000ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f8f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16932fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 0 for Classical Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X = X_lab,\n",
    "    Y = Y_lab,\n",
    "    Yhat = Yhat_lab,\n",
    "    X_unlabeled = X_unlab,\n",
    "    Yhat_unlabeled = Yhat_unlab,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19033bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 1 for Classical PPI Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 1,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = None for PPI++ Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ace3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pizza_eater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b404219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
