{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004f7a3a",
   "metadata": {},
   "source": [
    "# This is where I estimate baseline, naive, and ppi corrected inference parameters for each model classic, bert and gpt_zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "049369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load custom local packages\n",
    "from dataset_utils import dataframe_decorator\n",
    "from statistics_utils import *\n",
    "from ppi_plusplus_multi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac15d1",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63f8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../src/gpt_nlp/results_df.csv')\n",
    "mexico_knn = pd.read_csv('../data/results/mexico_KNN.csv')\n",
    "mexico_svm = pd.read_csv('../data/results/mexico_SVM.csv')\n",
    "mexico_nb = pd.read_csv('../data/results/mexico_NB.csv')\n",
    "mexico_bert = pd.read_csv('../data/results/mexico_bert.csv')\n",
    "mexico_gpt4zs = pd.read_csv('../data/results/mexico_gpt4_zs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2232eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_age_dist(site):\n",
    "    '''\n",
    "    This function takes in a site string, subsets the df to that site, \n",
    "    and returns the value counts distribution.\n",
    "    '''\n",
    "    sub_df = df[df['site']==site]\n",
    "    print(sub_df['age_yr'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74a3b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_yr\n",
      "53    29\n",
      "69    27\n",
      "72    27\n",
      "70    26\n",
      "55    26\n",
      "      ..\n",
      "15     2\n",
      "12     2\n",
      "94     1\n",
      "93     1\n",
      "14     1\n",
      "Name: count, Length: 87, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "check_age_dist('mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a72b640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X\n",
       "53    29\n",
       "69    27\n",
       "72    27\n",
       "70    26\n",
       "55    26\n",
       "      ..\n",
       "15     2\n",
       "12     2\n",
       "94     1\n",
       "93     1\n",
       "14     1\n",
       "Name: count, Length: 87, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mexico_gpt4zs['X'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc607",
   "metadata": {},
   "source": [
    "# Break into 80/20 split for Naive and PPI estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "46086d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(input_df, test_size=0.2):\n",
    "    '''\n",
    "    Takes input df which has three columns, [Y, X, Y_hat]\n",
    "    Subsets df to exclude 'unclassified' from Y_hat \n",
    "    returns data split unlabeled/labeled with a default 80/20 train/test split:\n",
    "        X (ndarray): Covariates corresponding to the gold-standard labels.\n",
    "        Y (ndarray): Gold-standard labels.\n",
    "        Yhat (ndarray): Predictions corresponding to the gold-standard labels.\n",
    "        X_unlabeled (ndarray): Covariates corresponding to the unlabeled data.\n",
    "        Yhat_unlabeled (ndarray): Predictions corresponding to the unlabeled data.    \n",
    "    '''\n",
    "    \n",
    "    # subset to drop unclassified\n",
    "    subset = input_df[input_df['Y_hat']!='unclassified']\n",
    "    print(subset.shape)\n",
    "\n",
    "    # 80/20 split on entire dataframe\n",
    "    train_df, test_df = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    \n",
    "    X = train_df['X'].values\n",
    "    Y = train_df['Y'].values\n",
    "    Yhat = test_df['Y_hat'].values # ?? WHAT IS THIS ??\n",
    "    X_unlabeled = test_df['X'].values\n",
    "    Yhat_unlabeled = test_df['Y_hat'].values\n",
    "    \n",
    "    return X, Y, Yhat, X_unlabeled, Yhat_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d8cb1933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306, 3)\n",
      "(1044, 3)\n",
      "(262, 3)\n"
     ]
    }
   ],
   "source": [
    "X_lab, Y_lab, Yhat_lab, X_unlab, Yhat_unlab = data_split(mexico_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "86008519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306, 3)\n",
      "(1044, 3)\n",
      "(262, 3)\n"
     ]
    }
   ],
   "source": [
    "X_lab, Y_lab, Yhat_lab, X_unlab, Yhat_unlab = data_split(mexico_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "61f61150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(530, 3)\n",
      "(424, 3)\n",
      "(106, 3)\n"
     ]
    }
   ],
   "source": [
    "X_lab, Y_lab, Yhat_lab, X_unlab, Yhat_unlab = data_split(mexico_gpt4zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16932fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 0 for Classical Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X = X_lab,\n",
    "    Y = Y_lab,\n",
    "    Yhat = Yhat_lab,\n",
    "    X_unlabeled = X_unlab,\n",
    "    Yhat_unlabeled = Yhat_unlab,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19033bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = 1 for Classical PPI Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 1,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhat = None for PPI++ Point Estimate\n",
    "ppi_plusplus_multi.ppi_multi_class_pointestimate(\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    lhat = 0,\n",
    "    coord = None,\n",
    "    optimizer_options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ace3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b404219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
